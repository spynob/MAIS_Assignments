{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fN-dse1NBQnk",
        "wfCjr-JrEJya",
        "Myn-42J9ACsH",
        "1LAqa9be_3vR",
        "rnIbpGe-Z52z",
        "W6CdJGpjcK2r"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spynob/MAIS_Assignments/blob/main/Assignment2_SPB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpWWCpEXiEda"
      },
      "source": [
        "# Assignment 2 ― Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9UW_dipiFw9"
      },
      "source": [
        "## 0. Introduction\n",
        "\n",
        "In this second assigment, we will explore another cornerstone of machine learning: supervised classification. We will be specifically classifying IMDB movie reviews by their positive (1) or negative (-1) score. To do this, we will first pre-process the raw data by cleaning and turning each review into a vector. Then, we will explore and fine-tune our use of the following learning algorithms for classification: naive Bayes classifiers, support vector machines, and random forests. \n",
        "\n",
        "* [Question 1.1](#scrollTo=m0QxxH3KngAg)\n",
        "* [Question 2.1](#scrollTo=6VWSBN37uXod)\n",
        "* [Question 2.2](#scrollTo=CuW0ahvaJtOt)\n",
        "* [Question 2.3](#scrollTo=co54Ubd5QJDN)\n",
        "* [Question 3.1](#scrollTo=fN-dse1NBQnk)\n",
        "* [Question 3.2](#scrollTo=wfCjr-JrEJya)\n",
        "* [Question 3.3](#scrollTo=l1iGVZtkE5fF)\n",
        "* [Question 4.1](#scrollTo=YKLBuWjmAKoJ)\n",
        "* [Question 5.1](#scrollTo=Myn-42J9ACsH)\n",
        "* [Question 5.2](#scrollTo=1LAqa9be_3vR)\n",
        "* [Question 6.1](#scrollTo=C144CYOeYPca) [Optional] \n",
        "* [Question 6.2](#scrollTo=rnIbpGe-Z52z) [Optional] \n",
        "* [Question 6.3](#scrollTo=W6CdJGpjcK2r) [Optional] \n",
        "\n",
        "\n",
        "\n",
        "$% latex commands for later use$\n",
        "$\\newcommand{\\R}{\\mathbb{R}}$\n",
        "$\\newcommand{\\B}{\\mathbb{B}}$\n",
        "$\\newcommand{\\argmax}{\\operatorname*{arg\\ max}}$\n",
        "$\\newcommand{\\given}{\\; \\vert \\;}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJHuG6PnQ-N"
      },
      "source": [
        "## 1. Importing Libraries and Data\n",
        "\n",
        "For this assignment, we will be using a dataset of IMDB reviews. The data consists of a csv file where the first column is a string containing a user review and the second column specifies whether the review was positve (1) o negative (-1). First, we will import any libraries that we might use.\n",
        "\n",
        "**Note:** You may use any library you would like unless specified otherwise. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0QxxH3KngAg"
      },
      "source": [
        "### Question 1.1 Importing Libraries\n",
        "\n",
        "Keep on adding in the section below any modules you use as you are completing the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81LFo0NUUIJA"
      },
      "source": [
        "import csv\n",
        "import random\n",
        "### Answer starts here ###\n",
        "from bs4 import BeautifulSoup\n",
        "### Answer ends here ###font"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "053rRiUAnsxQ"
      },
      "source": [
        "Let's download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4NfIOH0UXUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b99e860-302b-4927-cabf-cd3614c131df"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/McGillAISociety/BootcampAssignmentDatasets/master/data/assignment2/train_reviews.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 04:35:53--  https://raw.githubusercontent.com/McGillAISociety/BootcampAssignmentDatasets/master/data/assignment2/train_reviews.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33333854 (32M) [text/plain]\n",
            "Saving to: ‘train_reviews.csv’\n",
            "\n",
            "train_reviews.csv   100%[===================>]  31.79M   147MB/s    in 0.2s    \n",
            "\n",
            "2023-02-18 04:35:54 (147 MB/s) - ‘train_reviews.csv’ saved [33333854/33333854]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6P_kDBXtpgw"
      },
      "source": [
        "And create a function to print a review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-fshObCUX1-"
      },
      "source": [
        "def print_review(review, score):\n",
        "  print('--------------- Review with score of {} ---------------'.format(score))\n",
        "  print(review)\n",
        "  print('------------------------------------------------------')\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V_U9-Iwt0IP"
      },
      "source": [
        "Let's load the data and see what the first 10 reviews look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87qWNsF7U3QO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abde34e-d0c7-41d7-a61b-5287a0504b9b"
      },
      "source": [
        "with open('train_reviews.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file)\n",
        "  colnames = next(csv_reader)  # skip column names\n",
        "  data = list(csv_reader)\n",
        "\n",
        "for review, score in random.sample(data, 10):\n",
        "  print_review(review, score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------- Review with score of 1 ---------------\n",
            "One of the best comedians ever. I've seen this show about 10 times and will probably watch it at least 100 more. My friends and family quote from this DVD so often, you'd think we did nothing other than watch it. The beginning part about Alcatraz is a little bit slow, but either wade through it or zip on through to the part where Eddie is on stage. Watch for the \"Cake or Death\" part (Joking about the Church of England) and the \"Hitler/Pol Pot\" part (Hard to explain, just watch it). The best part of the show may be Eddie's facial expressions. He can really say a lot with his eyes. (Mascara, eyeliner, and eyeshadow probably help, huh?) Fair warning: Eddie does have a tendency to throw a lot of four-letter words in.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of -1 ---------------\n",
            "wow...this has got to be the DUMBEST movie I've ever seen. We watched it in english class...and this movie made ABSOLUTELY no sense. I would never, EVER watch this movie again...and my sympathy to those who have ever PAID to see it.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 1 ---------------\n",
            "This is one of the best movies I've ever seen. It has very good acting by Hanks, Newman, and everyone else. Definitely Jude Law's best performance. The cinematography is excellent, the editing is about as good, and includes a great original score that really fits in with the mood of the movie. The production design is also a factor in what makes this movie special. To me, it takes a lot to beat Godfather, but the fantastic cinematography displayed wins this contest. Definitely a Best Picture nominee in my book.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 1 ---------------\n",
            "Two sorcerers battle in the fourth dimension,one(Brian Thompson as a Kabal)trying to destroy the Earth,the other(Jeffrey Combs as a Anton Mordrid)trying to save it.\"Doctor Mordrid\" is an enjoyable fantasy fare which offers plenty of cheese.The plot is pretty silly and the gore is completely absent,but the film is very short and entertaining.So if you have enough time to kill give this one a look.My rating:7 out of 10.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 1 ---------------\n",
            "The Color Purple is about the struggles of life and the love that helps those people strongly affected by the struggles of life. Every character had an element of the color purple in them. The movie touches on love, lost, hope, hate, and triumph. whether it be Celie having lived through hell and losing her sister, and Shug coming into her life to show her love again, Albert not being man righting his wrong toward Celie, Shug shunned by her father and confesses to him in the end, Sofia and her stubborness good and bad, and even Nettie, they had their emptiness and hardship through the film but was overcome in the end and that's the sign of a good movie. Good Job to all the cast and crew.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of -1 ---------------\n",
            "Enough is enough...sometimes they just need to stop making movies based on a concept that is long dead. The first Tremors movie was great. The second one was ridiculous. The third one was nauseating. The tv series was depressingly awful. And this movie just drives the stake deeper.<br /><br />Basically another excuse for cheap computer effects and puppetry, now we have the series set in the Wild West, in the 1800's, and they fight graboids. Like a rehash of the first one, they have to learn how to beat them all over again. Mildly entertaining I suppose. Otherwise this straight-to-video release, just like Tremors 2 and 3, is just going way too far. Oh and I continue to wonder how there is never any record of these events taking place...did they just simply forget to record this unprecedented event? I think something like this would be history-making, so our pals in the first film wouldn't be so unprepared. <br /><br />Movies like this that ruin the original just make me crazy. Avoid this garbage.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 1 ---------------\n",
            "I absolutely loved this movie. I bought it as soon as I could find a copy of it. This movie had so much emotion, and felt so real, I could really sympathize with the characters. Every time I watch it, the ending makes me cry. I can really identify with Busy Phillip's character, and how I would feel if the same thing had happened to me.<br /><br />I think that all high schools should show this movie, maybe it will keep people from wanting to do the same thing. I recommend this movie to everybody and anybody. Especially those who have been affected by any school shooting.<br /><br />It truly is one of the greatest movies of all time.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of -1 ---------------\n",
            "Having searched for this movie high and low, I actually found it when I least expected, playing on the Sundance Channel very early in the morning one day. Why I searched endlessly for a small vanity project that Chuck Barris that was made during the last waning years of the TV show, I haven't a clue. The film is simply put horrible. The scripted part that deals with a week that is. Of course the highlight of the film is seeing the real performers that were \"too hot for TV\" or rejected for some reason or other. That part is still horrid, but campy bad which was enjoyable in it's own way. Now that I saw what I sought after for so long will I watch it again in my lifetime? Resoundingly NO!! Do yourself a favor and just watch the MUCH MUCH better \"Confessions of a Dangerous Mind\" or find old copies of the actual show. The girl act where there just lick popsicles provocatively was fun, but having to endure seeing Jay P. Morgon flash the audience has in all likelihood made me sterile. In hindsight, I'm so very happy that this was massive flop, for if it was a massive hit, there could have been a \"The $1.98 Beauty Show Movie\" and THAT my friends would surely have brought upon the Apocalypse.<br /><br />My Grade: D\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 1 ---------------\n",
            "It seems to be a perfect day for swimming. A normal family wants to gain advantage from it and takes a trip to the beach. Unfortunately it happens that the father is trapped under a pier and neither his wife nor the small son is able to help him out of this - whereas the tide is rising. The woman (Barbara Stanwyck) takes the car and searches for help.<br /><br />John Sturges' short movie (69 minutes) is powerful because of unanswered questions. Stanwyck finds a guy who could help, but there is a price she has to pay for this. There is a double question the movie poses. How far would you go to help the man that you love, and on the other hand - observing Stanwyck's behaviors towards the stranger - does she really love her husband? Like a good short story this movie leaves the viewer to himself with questions he can only answer himself.\n",
            "------------------------------------------------------\n",
            "\n",
            "--------------- Review with score of 1 ---------------\n",
            "I've read a number of reviews on this film and I have to say \"What is wrong with you people?!?!\" This was an excellent film! I thought this film was superb from start to finish and the story was extremely well told. I'm convinced that the people that didn't like this film weren't paying very good attention to the film. There are a number of very important scenes that if you aren't paying attention you will be confused and the following scenes may not make sense. I urge anyone who didn't like this film to watch it again and watch it alone so that you can truly pay attention. The story made perfect sense to me and as I said, was very well told. Every scene in the film has a point and everything fits together at the end of the film.<br /><br />All the actors did a fantastic job! Sean Connery was very good in his role as always. Laurence Fishburne was superb as Tanny Brown, playing a very interesting character. Kate Capshaw was a nice touch as well, and looks fantastic. Blair Underwood was a pleasant surprise, I didn't really expect anything great from him, but he pulled off a great performance. Ed Harris was the real gem in the film. He plays a truly sick individual and really makes you see how disturbed his character is. Watch his eyes in his scenes, just superb!!! Also, there is a very young Scarlett Johansson (as Kate) in one of her first roles...not a bad place to start. Excellent cast in this film!<br /><br />I would strongly recommend this film to anyone that likes any of the cast members or just likes thrillers. This is a great film and should be seen. Don't listen all these other people's opinions, go see the movie and come to your own conclusions. I hope that you will see the film, and I hope that you enjoy it as much as I do. Thanks for reading,<br /><br />-Chris\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7qKVCAst-kv"
      },
      "source": [
        "## 2. Preprocessing\n",
        " We will be converting our data into a binary bag-of-words representation (Google \"binary bag-of-words\"). To do this, we will perform two steps beforehand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VWSBN37uXod"
      },
      "source": [
        "### Question 2.1 Cleaning the train data\n",
        "Create a function called `clean`, which takes a string and then:\n",
        "\n",
        " 1. lower-case all words \n",
        " 2. only keeps letters and spaces\n",
        " \n",
        "\n",
        " We also need to get rid of [HTML tags](https://www.javatpoint.com/html-tags) as they do not hold valuable information for classifying the review. A quick Google search on removing HTML tags with `regular expressions` will show you how to do this! \n",
        "  \n",
        "  For example, the following review...\n",
        "  \n",
        "  >`This was the WORST movie I have EVER SEEN!! <br/>`\n",
        "  \n",
        "  ...will be cleaned to become:\n",
        "  \n",
        "  >`this was the worst movie i have ever seen`\n",
        "  \n",
        "   Of course, you could do more pre-processing steps if you would like, such as lemmatization, stemming, etc... but TOTALLY OPTIONAL! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAje0IKVU2-c"
      },
      "source": [
        "def clean(review):\n",
        "  ### Start of Answer ###\n",
        "  review.lower()\n",
        "  return BeautifulSoup(review, \"lxml\").text\n",
        "  ### End of Answer ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwT4zB4RIodc"
      },
      "source": [
        "Test your function with this example string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBnmJ_jIU27c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b7c022-f814-4cb4-eeda-7160d0e4ce84"
      },
      "source": [
        "print(clean(\"This was the WORST movie I have EVER SEEN!! <br/> \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This was the WORST movie I have EVER SEEN!!  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ3csQCzI27I"
      },
      "source": [
        "Now, we'll use the function to clean the whole dataset. We'll also turn the scores from strings to integers while we're at it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRkQWUIvYfnO"
      },
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "for review, score in data:\n",
        "  X_train.append(clean(review))\n",
        "  y_train.append(int(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuW0ahvaJtOt"
      },
      "source": [
        "### Question 2.2 Picking features\n",
        "\n",
        "We now need to turn each review into vectors. We will pick the 10,000 most recurring words in the train set as features\n",
        "\n",
        "Using those 10,000 features, create a function called `vectorize` which will take a string as an input, and convert it to a vector using the binary bag of words representation.\n",
        "\n",
        "For example, the string `\"This movie made me cry\"` will become a vector of size 10,000 with 5 elements being 1 (assuming each word is part of the 10,000 most common) and 9995 being 0, that it, is i will look something like\n",
        "\n",
        " > `[0, 0, ..., 0, 1, 0, ..., 0, 1, 0..., 0, 1, 0, ..., 0, 1, 0 ..., 0, 1, 0, ..., 0, 0]`\n",
        " \n",
        " In order to accomplish this task, you will\n",
        " \n",
        " 1. write a `get_vocab` function which takes as an argument a list of (cleaned) reviews and the vocabulary size and outputs the a list of size `vocab_size` containing the most common words.\n",
        " 2. write a `vectorize` function which takes as an argument a review and the vocabulary and turns the review into its binary bag of words representation.\n",
        " 3. use the `vectorize` function to create a new variable called `X_train_vect` which will contain the bag-of-words representation of each data point contained in the `X_train` variable rather than its string representation.\n",
        "\n",
        "**Note**: each step should take no longer than 30 seconds to run! Make sure you implement the functions efficiently, taking the time complexity of your solution into account. `collections.Counter` may be of use to you (see [here](https://realpython.com/python-dicts/) for an explanation of python dictionaries and [here](https://docs.python.org/3/library/collections.html#collections.Counter) for the documentation for `Counter`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa0E04tFYf6x"
      },
      "source": [
        "def get_vocab(reviews, vocab_size):\n",
        "  ### Answer starts here ###\n",
        "  review = [j for i in review for j in i.split]\n",
        "  ### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def power(x,n):\n",
        "  return math.pow(x,n)\n",
        "  "
      ],
      "metadata": {
        "id": "wvx88L_8vBXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E38yzbclLXs3"
      },
      "source": [
        "Test your function with the following code. The `vocabulary` variable should have a length of 10,000 and the most common words should be \"the\", \"and\", \"a\", etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD9Jz8zpYfru",
        "outputId": "f0e6dad1-0e5c-47d9-ee60-878ef277678d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "num_features = 10000\n",
        "vocabulary = get_vocab(X_train, num_features)\n",
        "print(len(vocabulary))\n",
        "print(vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-87a5d261ed74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1664b0443b7e>\u001b[0m in \u001b[0;36mget_vocab\u001b[0;34m(reviews, vocab_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m### Answer starts here ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m### Answer ends here ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'review' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKBRgc4UYfjf"
      },
      "source": [
        "def vectorize(review_string, vocab):\n",
        "  ### Answer starts here ###\n",
        "  \n",
        "  ### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7UHbv8SLiz7"
      },
      "source": [
        "Test your function with the following input. The vector should have four \"1\"s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ohyWmFCeknM"
      },
      "source": [
        "vector = vectorize(\"the and a of zyxw\", vocabulary)\n",
        "print(vector)\n",
        "print(sum(vector))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn4UER2SL96P"
      },
      "source": [
        "Now, vectorize the whole dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUU0n_zsMvfv"
      },
      "source": [
        "### Answer starts here ###\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4U6rj0PQUjE"
      },
      "source": [
        "for i in range(5):\n",
        "  print_review(X_train_vect[i], y_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzNo-5oNRXz6"
      },
      "source": [
        "For convenience, we will write a function called `preprocess_sample_point` which takes as input a single raw review and ouputs its binary bag-of-words representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO9Ge_lwMwEP"
      },
      "source": [
        "def preprocess_sample_point(review, vocab):\n",
        "  return vectorize(clean(review), vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Wg-6UKrH8t"
      },
      "source": [
        "vectorized_review = preprocess_sample_point(\n",
        "    'The movie was not bad, it was really good!', vocabulary)\n",
        "print(sum(vectorized_review))\n",
        "print(vectorized_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co54Ubd5QJDN"
      },
      "source": [
        "### Question 2.3 Preparing the test set\n",
        "\n",
        "Now that we have defined a cleaning function and extracted the features from the train set, we are ready to preprocess the test set. Implement the `preprocess` function below such that it:\n",
        "\n",
        "1. Loads the raw data from a csv file \n",
        "2. Cleans and vectorizes the reviews\n",
        "3. Converts the scores to `int`\n",
        "4. Returns the data into a  `(X_test, y_test)` tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBoumQUSMvXD"
      },
      "source": [
        "def preprocess(csv_filename, vocab):\n",
        "  ### Answer starts here ###\n",
        "  with open(csv_filename) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    colnames = next(csv_reader)  # skip column names\n",
        "    data = list(csv_reader)\n",
        "  \n",
        "  X = []\n",
        "  y = []\n",
        "  for review, score in data:\n",
        "    X.append(vectorize(clean(review), vocab))\n",
        "    y.append(int(score))\n",
        "\n",
        "  return (X, y)\n",
        "  ### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t84s9ir_ekgA"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/McGillAISociety/BootcampAssignmentDatasets/master/data/assignment2/test_reviews.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYdinHq9UYWU"
      },
      "source": [
        "X_test, y_test = preprocess('test_reviews.csv', vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGKPj6iozEtS"
      },
      "source": [
        "for i in range(5):\n",
        "  print_review(X_test[i], y_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEIhyms7N3oo"
      },
      "source": [
        "\n",
        "##3. Naive Bayes\n",
        "Later in this assignment, we will use pre-existing implementations of two types of models: random forests and support vector machines. However, we will start first by implementing from sratch a third type of classifier: naive Bayes classifers. \n",
        "\n",
        "Naive Bayes classifiers are part of a larger family of classifiers which are called 'probabilistic classifiers': not only do they try to predict classes given features, but they also estimate probability distributions over a set of classes.\n",
        "\n",
        "First, let's will go over some definitions:\n",
        "\n",
        "**Definition:** A *prior probability* is the likelihood of an event given no further assumptions. For instance, the probability that it's raining is relatively low.\n",
        "\n",
        "**Definiton:** A *posterior probability* or *conditional probability* is the likelihood of an event given that some other event has occurred. For instance, the probability that it's raining given that there are clouds is higher than if we don't make that assumption.\n",
        "\n",
        "Now we will go over some motivation:\n",
        "\n",
        "For the purpose of argument, imagine we had access to the probability distribution $\\Pr$. That is, we know how likely features and classes are. For example, $\\Pr(x_1 = 1)$ is the probability that the most common word, i.e. \"the\", is in a random movie review. Presumably, this probability is relatively high. As a second example, $\\Pr(y = 1)$ is the probability that a random movie review is 'good'. \n",
        "\n",
        "Since we hypothetically have access to the whole probability distribution, we also know conditional probabilities. For instance, we would know $\\Pr(y = -1 \\; \\vert \\; x_1 = 0)$, which is the probability that a random review is 'bad', given that it does not contain the word \"the\".\n",
        "\n",
        "Given a probability distribution, we can find an optimal classifier which simply picks the class which maximizes the probability that we will see that class given the observed features, in other words our classifier $f: \\B^n \\to \\B$ is given by:\n",
        "\n",
        "$$ f(x_1, \\ldots, x_n) = \\argmax_{c \\in \\B} \\Pr(y = c \\given x_1, \\ldots, x_n ).$$\n",
        "\n",
        "Where $\\argmax$ returns the element in $\\B$ which maximizes the expression to its right, and $\\B$ is the set with two elements, $\\{-1, 1\\}$. For example, we have\n",
        "$$ \\argmax_{x \\in \\R} (x - x^2) = \\frac 1 2, $$\n",
        "since $\\frac 1 2$ maximizes the expression $x - x^2$.\n",
        "\n",
        "It would be great if we had access to the probability distribution $\\Pr$, but unfortunately we don't in almost every case. This means we wish to try to estimate it given some samples, i.e. the training data.\n",
        "\n",
        "However, we run into another issue: estimating the probability distribution is computionally expensive. Therefore, we assume that the different features are independent from one another. This is called the *naive conditional independence assumption*. In other words, we assume that\n",
        "\n",
        "$$ \\forall i \\in \\{1, \\ldots, n\\} : \\Pr (x_i \\given y, x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n) = \\Pr(x_i \\given y).$$\n",
        "\n",
        "Using Bayes' Theorem, we can simplify the conditional independence assumption to:\n",
        "\n",
        "$$\\Pr(y \\given x_1, \\ldots, x_n) = \\frac{\\Pr(y) \\prod_{i=1}^n \\Pr(x_i \\given y)}{\\Pr(x_1, \\ldots, x_n)}.$$\n",
        "\n",
        "However, we can observe that the denominator is constant for a given input, so it's not actually necesarry to estimate it if all we want is to find the class with the maximum posterior probability. In other words,\n",
        "\n",
        "$$ \\Pr(y \\given x_1, \\ldots, x_n) \\propto \\Pr(y) \\prod_{i=1}^n \\Pr(x_i \\given y), $$\n",
        "\n",
        "so, our classification rule becomes\n",
        "\n",
        "$$ f(x_1, \\ldots, x_n) = \\argmax_{y \\in \\B} \\Pr(y) \\prod _{i=1}^n \\Pr(x_i \\given y).$$\n",
        "\n",
        "Where $\\propto$ means \"proportional to\" and  $\\prod_{i = 1}^n g(i)$ is like summation $\\left(\\sum_{i=1}^n g(i)\\right)$, except that addition is replaced with multiplication. For example,\n",
        "\n",
        "$$\\prod_{i = 1}^5 i^2 = 1^2 \\cdot 2^2 \\cdot 3^2 \\cdot 4^2 \\cdot 5^2.$$\n",
        "\n",
        "**Note**: To estimate prior and conditional probabilities, we use the ratios of occurence counts found in the dataset. For example, to estimate $\\Pr(x_1 = 0 \\; \\vert \\; y = -1)$, we have to calculate the number of instances of class -1 for which $x_1 = 0$ and divide them by the number of instances of class -1.\n",
        "\n",
        "**Note**: The naive independence assumption is usually false in practice for most features. Therefore, the resulting estimated probability distribution is usually a bad approximation of the true distribution. However, the resulting classifier often has a good performance, depending on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN-dse1NBQnk"
      },
      "source": [
        "### Quesiton 3.1 Estimating the Probability Distribution\n",
        "\n",
        "It would be expensive to re-estimate prior and posterior probabilities every time, so we should save probabilities in memory.\n",
        "\n",
        "Thus, you will need to save\n",
        "1. $\\Pr(y)$ for each $y \\in \\B$, and\n",
        "2. $\\Pr(x_i = u \\; \\vert \\; y)$ for each $ i \\in \\{1, \\ldots, n\\}$, $u \\in \\mathbb{B}$ and $y \\in \\mathbb{B}$.\n",
        "\n",
        "Remember that you are *estimating* the probabilities using the training set only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-KrWs3DER_R"
      },
      "source": [
        "### Answer starts here ###\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfCjr-JrEJya"
      },
      "source": [
        "### Question 3.2 Creating the Naive Bayes Classifier\n",
        "\n",
        "Create a function called `naive_bayes` which will take as input a list of features $x_1, \\ldots, x_n$ and outputs the class with the largest posterior probability given the input features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfKW9cZs4x-N"
      },
      "source": [
        "def naive_bayes(vec):\n",
        "  ### Answer starts here ###\n",
        "  \n",
        "  ### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1iGVZtkE5fF"
      },
      "source": [
        "### Question 3.3 Measuring Performance\n",
        "\n",
        "Using the naive Bayes classifier, predict the classes for each sample point in the training set as well as the test set and print accuracies.\n",
        "\n",
        "**Note:** You should get train and test accuracies of about 80-85%.\n",
        "\n",
        "**Hint:** You can use the `accuracy_score` function provided by `sklearn.metrics`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SjmIjYPSW10"
      },
      "source": [
        "### Answer starts here ###\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ZbGmYj2WnZ"
      },
      "source": [
        "print(naive_bayes(preprocess_sample_point(\n",
        "    'Terrible. Horrible. Boring. This movie is bad', vocabulary)))\n",
        "\n",
        "print(naive_bayes(preprocess_sample_point(\n",
        "    'This movie was pretty good', vocabulary)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-azU55RKk-s"
      },
      "source": [
        "## 4. Support Vector Machines\n",
        "\n",
        "Quick recap of SVM: A support vector classifier tries to find the best separating hyperplane through the data. If the data is linearly separable, it finds a hyperplane that maximizes the margin. If it isn't, the classifier tries to minimize the cost associated with misclassifying points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKLBuWjmAKoJ"
      },
      "source": [
        "### Question 4.1 Creating a Support Vector Classifier\n",
        "Using `scikit-learn`, create a support vector classifier for our review data.\n",
        "\n",
        "1. Use `scikit-learn` to create a linear support vector classifer (name it `svm_clf`) (Make sure to use svm.LinearSVC and not svm.SVC as LinearSVC is more adapted to large datasets)\n",
        "2. Fit the model to our training set\n",
        "3. Print training accuracy\n",
        "4. Print test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11h5qZgArCLb"
      },
      "source": [
        "### Answer starts here ###\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRk2DxHptzmE"
      },
      "source": [
        "print(svm_clf.predict([preprocess_sample_point(\n",
        "    'Boring. Such a bad movie. It was terrible and predictable', vocabulary)]))\n",
        "\n",
        "print(svm_clf.predict([preprocess_sample_point(\n",
        "    'I really liked this movie, it\\'s great!', vocabulary)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9sFSUNKscM"
      },
      "source": [
        "## 5. Random Forests\n",
        "\n",
        "Random forests are a type of ensemble classifier, i.e. they are made up of a number of 'weak' learners where the final classification is a combination of the classifications of each learner. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myn-42J9ACsH"
      },
      "source": [
        "### Question 5.1 Creating a Random Forest Classifier\n",
        "Using `scikit-learn`, create a radom forest classifier for our review data.\n",
        "\n",
        "1. Use `scikit-learn` to create a random forest classifier (name it `rfc`)\n",
        "2. Fit the model to our training set\n",
        "3. Print training accuracy\n",
        "4. Print test accuracy\n",
        "\n",
        "\n",
        "\n",
        "Be sure to check the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Try to play around with the hyperparameters to see if you can get higher accuracy. Specifically, try finding good values for `n_estimators`, `min_samples_split`, `max_depth` and `max_features`. Try to get accuracies close to the SVM's!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddm3xZiNZkXu"
      },
      "source": [
        "### Answer starts here ###\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPcnt6mI2g5e"
      },
      "source": [
        "print(rfc.predict([preprocess_sample_point(\n",
        "    'Boring. This movie is terrible', vocabulary)]))\n",
        "\n",
        "print(rfc.predict([preprocess_sample_point(\n",
        "    'This movie was pretty good', vocabulary)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LAqa9be_3vR"
      },
      "source": [
        "### Question 5.2 Manual hyperparameter tuning\n",
        "\n",
        "Tell us about your hyperparamter tuning in a few sentences! What was your approach? Which paramters did you try changing? Were you able to improve your accuracies? (no right answer, just tell us what you experiemented with!)\n",
        "\n",
        "#############\n",
        "\n",
        " **ANSWER HERE**\n",
        "\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP4D-1PjSVfA"
      },
      "source": [
        "## 6. [Optional] Tuning hyperparameters with GridSearchCV\n",
        "In this **optional** section, we will explore how to exhaustively tune the hyperparameters of a new classifier using `sklearn's` [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C144CYOeYPca"
      },
      "source": [
        "### Question 6.1 Creating an Adaboost classifier\n",
        "In this section, we will be tuning the hyperparameters of an `adaboost` model. \n",
        "\n",
        "As its `sklearn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) mentions, this classifier \"begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\" \n",
        "\n",
        "For this question, simply create an instance of the classifer and name it `abc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhuzkrzfZitf"
      },
      "source": [
        "### Answer starts here ###\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnIbpGe-Z52z"
      },
      "source": [
        "### Question 6.2 Finding the best parameters\n",
        "In order to use GridSearchCV, we need to provide it with a classifier (here, `abc`) and lists of different values for the hyperparameters we want to tune. GridSearchCV will run a fit using each possible parameter and get crossvalidation scores. \n",
        "\n",
        "For this question, use the GridSearchCV documentation to:\n",
        "1. Create a dict of parameters to tune with their respective list of values\n",
        "2. Create an instance of GridSearchCV with `abc` as the estimator and the following arguments: `cv=5, refit=False, verbose=3`\n",
        "3. Print the resulting scores and determine which parameters are best!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CZJsxsqa1bC"
      },
      "source": [
        "### Answer starts here ###\n",
        "\n",
        "### Answer ends here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6CdJGpjcK2r"
      },
      "source": [
        "### Question 6.3 Final result with best parameters\n",
        "Using the best parameters found in the previous question, reinstantiate an `adaboost` classifier, fit it to the train data, and print the the accuracy score on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acx0uXvpeycS"
      },
      "source": [
        "### Answer starts here ####\n",
        "\n",
        "### Answer ends here ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOsnRc2Q6ytj"
      },
      "source": [
        "## 7. Recap and conclusion\n",
        "Congratulations on reaching the end of Assignment 2! We hope you enjoyed it. \n",
        "\n",
        "Here's a recap of tasks and concepts explored in this assignment:\n",
        "\n",
        "\n",
        "1.   **Preprocessing**\n",
        "*   Cleaning train data: removing punctuation and HTML tags\n",
        "*   Basic feature engineering: vectorizing data, bag-of-words representation \n",
        "*   Using learned vocabulary to preprocess test data\n",
        "\n",
        "Preprocessing and data representation are very important in Natural Language Processing (NLP) projects. For more advanced preprocessing techniques, we highly recommend checking online the concepts of **stemming**, **n-gramming**, **stopwords removal**!\n",
        "\n",
        "2.   **Naive Bayes**\n",
        "*   Naive Bayes: recap of theory\n",
        "*   Implementing model from scratch\n",
        "\n",
        "Find out more about Naive Bayes implementations in `sklearn`: [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html), and more!\n",
        "\n",
        "3.   **Support Vector Machine**\n",
        "*   SVM: brief recap of theory and testing\n",
        "*   First introduction to `sklearn`: using already implemented models to classify data\n",
        "\n",
        "4.   **Random Forests**\n",
        "*   Random Forests: brief recap of theory and testing\n",
        "*   Introduction to hyperparameter tuning\n",
        "\n",
        "The default parameters are not always good enough! Find out more about how to tune hyperparameters in `sklearn` [here](https://scikit-learn.org/stable/modules/grid_search.html) or do the optional section 6 of this assignment. \n",
        "\n",
        "5.   **Tuning Adaboost with GridSearchCV**\n",
        "*   Adaboost: bried recap of theory and testing \n",
        "*   Tuning hyperparameters using GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7okgCQqFnxm"
      },
      "source": [
        "## 8. Submission\n",
        "\n",
        "To submit your work, copy this colab, set the permissions of your copy to \"anyone with the link can view\" and copy paste the link to your colab on MyCourses submission."
      ]
    }
  ]
}